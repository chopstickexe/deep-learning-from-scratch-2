{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMFLKIJ78LhU2h90Qju4PXh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chopstickexe/deep-learning-from-scratch-2/blob/master/ch05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMKwPjfsSDLh",
        "colab_type": "text"
      },
      "source": [
        "# 下準備\n",
        "\n",
        "$$\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\mat}[1]{\\mathbf{#1}}\n",
        "$$\n",
        "\n",
        "## 数式の表記\n",
        "\n",
        "変数: 小文字イタリック $x$\n",
        "\n",
        "定数: 大文字イタリック $X$\n",
        "\n",
        "ベクトル: 小文字ローマン体太字 $\\vect{x}$\n",
        "\n",
        "行列: 大文字ローマン体太字 $\\mat{X}$\n",
        "\n",
        "## 公式実装のclone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpcQLFeCSHyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "e2c8ebd2-aba6-493e-e239-f4cca3b1002e"
      },
      "source": [
        "!git clone --depth=1 https://github.com/oreilly-japan/deep-learning-from-scratch-2.git\n",
        "import sys \n",
        "sys.path.append('deep-learning-from-scratch-2')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 13), reused 14 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzFKWPwHSsRu",
        "colab_type": "text"
      },
      "source": [
        "# 5章 リカレントニューラルネットワーク（RNN）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-405UsE6UiV5",
        "colab_type": "text"
      },
      "source": [
        "## 5.3 RNNの実装（`RNN`, `TimeRNN`クラス）\n",
        "\n",
        "順伝播は一つ前の時刻$t-1$の隠れ変数$\\vect{h_{t-1}}$と，時刻$t$の入力$\\vect{x_t}$を用いて以下のように求める．\n",
        "\n",
        "$$\n",
        "\\vect{h_t} = \\mathrm{tanh}(\\vect{h_{t-1}} \\mat{W_h} + \\vect{x_t} \\mat{W_x} + b)\n",
        "$$\n",
        "\n",
        "なお，$y = \\mathrm{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e~{-x}}$の微分は$1-y^2$（付録A参照）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO4DucPfUkkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN:\n",
        "  def __init__(self, Wx, Wh, b):\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, x, h_prev):\n",
        "    Wx, Wh, b = self.params\n",
        "    t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "    h_next = np.tanh(t)\n",
        "\n",
        "    self.cache = (x, h_prev, h_next)\n",
        "    return h_next\n",
        "\n",
        "  def backward(self, dh_next):\n",
        "    Wx, Wh, b = self.params\n",
        "    x, h_prev, h_next = self.cache\n",
        "    \n",
        "    dt = dh_next * (1 - h_next ** 2)  # d(tanh(x))/dx = 1 - y^2\n",
        "    db = np.sum(dt, axis=0)  # 一つのベクトル値をミニバッチ分複製して順伝播したので，逆伝播のときはsumする．\n",
        "    dWh = np.dot(h_prev.T, dt)\n",
        "    dh_prev = np.dot(dt, Wh.T)\n",
        "    dWx = np.dot(x.T, dt)\n",
        "    dx = np.dot(dt, Wx.T)\n",
        "\n",
        "    self.grads[0][...] = dWx\n",
        "    self.grads[1][...] = dWh\n",
        "    self.grads[2][...] = db\n",
        "\n",
        "    return dx, dh_prev    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R0rDWXhUwFR",
        "colab_type": "text"
      },
      "source": [
        "時刻$0, ..., T$分のRNNレイヤをまとめてTime RNNレイヤとして実装する．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P2FLQ0sdcqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TimeRNN:\n",
        "  def __init__(self, Wx, Wh, b, stateful=False):  \n",
        "    # stateful=Trueの場合には前にforwardを計算して得られたhを記憶するようにする（長い時系列をTruncateしてミニバッチ化するため）\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "    self.layers = None\n",
        "\n",
        "    self.h, self.dh = None, None\n",
        "    self.stateful = stateful\n",
        "\n",
        "  def set_state(self, h):\n",
        "    self.h = h\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.h = None\n",
        "\n",
        "  def forward(self, xs):\n",
        "    Wx, Wh, b = self.params\n",
        "    N, T, D = xs.shape\n",
        "    D, H = Wx.shape\n",
        "\n",
        "    self.layers = []\n",
        "    hs = np.empty((N, T, H), dtype='f')  # zerosと違って0で初期化しない（zerosよりも高速の場合がある）\n",
        "\n",
        "    if not self.stateful or self.h is None:\n",
        "      # 初期化\n",
        "      self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = RNN(*self.params)  # RNNクラスのinitにWx, Wh, bを展開して代入している（つまりこの辺の重みは全部共有）\n",
        "      self.h = layer.forward(xs[:, t, :], self.h)  # self.hはtごとに更新される（stateful=Trueであれば、0-Tのloopを抜けて次にforwardが呼ばれたときも引き継がれている\n",
        "      hs[:, t, :] = self.h\n",
        "      self.layers.append(layer)\n",
        "\n",
        "    return hs\n",
        "\n",
        "  def backward(self, dhs):  # 逆伝播のときの入力はdh_tとdh_{t+1}になるが、dh_{t+1}はそのままクラス変数として引き継がれてくる\n",
        "    Wx, Wh, b = self.params\n",
        "    N, T, H = dhs.shape\n",
        "    D, H = Wx.shape\n",
        "\n",
        "    dxs = np.empty((N, T, D), dtype='f')  # 0で初期化しない行列の初期化\n",
        "    dh = 0  # Trancated BPTTに基づいてRNNを学習しているため、t=Tのときにdh_{T+1}は伝播させない（そのため、dh=self.dhではない）\n",
        "    grads = [0, 0, 0]\n",
        "    for t in reversed(range(T)):\n",
        "      layer = self.layers[t]\n",
        "      dx, dh = layer.backward(dhs[:, t, :] + dh)  # 分岐して順伝播しているのでsumで逆伝播\n",
        "      dxs[:, t, :] = dx\n",
        "\n",
        "      for i, grad in enumerate(layer.grads):\n",
        "        grads[i] += grad  # Wx, Wh, bはすべてのtに対して同じものを使って（＝分岐して）順伝播しているので、sumで逆伝播\n",
        "    \n",
        "    for i, grad in enumerate(grads):\n",
        "      self.grads[i][...] = grad  # 0-Tの逆伝播が終わったら、勾配を更新\n",
        "\n",
        "    self.dh = dh  # いまはメンバ変数に保持するだけで何もしないが、7章のseq2seqでこれを使う\n",
        "\n",
        "    return dxs\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNIBWVMZtFXt",
        "colab_type": "text"
      },
      "source": [
        "## 5.4 時系列データを扱うレイヤの実装（`TimeEmbedding`, `TimeAffine`, `TimeSoftmaxWithLoss`クラス）\n",
        "\n",
        "5.3で実装したTime RNNレイヤの前後にレイヤをくっつけて、t=0～T-1のEmbedding -> RNN（t=0～T-2の隠れベクトルを入力とする） -> Affine -> Softmaxでt=1～Tの単語の確率を出力する，言語モデル（RNNLM）を実装する．\n",
        "\n",
        "t=0～T-1をまとめて処理するTime RNNと同様に，Time Embedding, Time Affine, Time Softmax With Lossレイヤを実装する．（`common/time_layers.py` 参照）\n",
        "\n",
        "- `TimeEmbedding`: $\\vect{x_{0}}, ..., \\vect{x_{T-1}}$を`xs`として受け取り，単純に各$t$に対してforward, backwardを施す．\n",
        "- `TimeAffine`: `TimeEmbedding`と同様に$\\vect{x_{0}}, ..., \\vect{x_{T-1}}$を`x`として受け取るが，まじめに各$t$に対して$\\vect{x_t} \\mat{W} + \\vect{b}$を計算するのではなく，バッチ内の全$\\vect{x}$を各行とする行列を作り（行: バッチ数N×T，列: データ次元数D）、$\\mat{W}$と一回行列積を求めれば済むようにしている．backwardも同様．\n",
        "- `TimeSoftmaxWithLoss`: `TimeAffine`と同様に，N×T行の行列を作ってまとめて処理．損失値は$t=0, ..., \\mathrm{T}$およびバッチ$N$の中で平均を取る．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemVQ8PHxsue",
        "colab_type": "text"
      },
      "source": [
        "## 5.5 RNNLMの学習と評価（`SimpleRnnlm` クラス）\n",
        "\n",
        "5.3, 5.4のクラスを使ってSimpleRnnlmクラスを実装する．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKDMpHtd3aRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from common.time_layers import *\n",
        "\n",
        "class SimpleRnnlm:\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    rn = np.random.randn\n",
        "\n",
        "    # 重みの初期化\n",
        "    embed_W = (rn(V, D) / 100).astype('f')  # 突然100で割っているが、前章までで0.01かけてたのと同じ\n",
        "    rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')  # 前の層のノード数がnのとき，1/\\sqrt{n}の標準偏差を持つ分布で初期化する（Xavierの初期値の簡易実装．本当は次層のノード数も考慮するらしい）\n",
        "    rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
        "    rnn_b = np.zeros(H).astype('f')\n",
        "    affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "    affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "    # レイヤの生成\n",
        "    self.layers = [\n",
        "      TimeEmbedding(embed_W),\n",
        "      TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True), # stateful=Trueなので、順伝播のときにバッチの切れ目で前のバッチで計算された前時刻の隠れ状態を引き継ぐ\n",
        "      TimeAffine(affine_W, affine_b)\n",
        "    ]\n",
        "    self.loss_layer = TimeSoftmaxWithLoss()\n",
        "    self.rnn_layer = self.layers[1]\n",
        "\n",
        "    # すべての重みと勾配をリストにまとめる\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def forward(self, xs, ts):\n",
        "    for layer in self.layers:\n",
        "      xs = layer.forward(xs)  # 4章までの実装と同じ\n",
        "    loss = self.loss_layer.forward(xs, ts)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dout = self.loss_layer.backward(dout)\n",
        "    for layer in reversed(self.layers):\n",
        "      dout = layer.backward(dout)\n",
        "    return dout\n",
        "\n",
        "  def reset_state(self):  # RNNのstateをリセット\n",
        "    self.rnn_layer.reset_state()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryVb2iw1IPRS",
        "colab_type": "text"
      },
      "source": [
        "Penn Treebankデータセットを用いた学習コードは以下の通り（`ch05/train_custom_loop.py`）．\n",
        "\n",
        "一般に言語モデルの評価はperplexity $e^L$ で行われる．このとき$L$はNNの損失関数と同じ交差エントロピー誤差で，perplexityは低いほど良い．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od00Xt_wIUes",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bde9531f-edd4-4a64-fabc-9cb1bea93ef7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from common.optimizer import SGD\n",
        "from dataset import ptb\n",
        "\n",
        "batch_size = 10\n",
        "wordvec_size = 100\n",
        "hidden_size = 100\n",
        "time_size = 5  # Truncated BPTTの単位時間\n",
        "lr = 0.1\n",
        "max_epoch = 100\n",
        "\n",
        "# 学習データの読み込み\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "corpus_size = 1000  # とりあえず1000語読み込む\n",
        "corpus = corpus[:corpus_size]\n",
        "vocab_size = int(max(corpus) + 1)\n",
        "\n",
        "xs = corpus[:-1]  # 入力語全部（0～1000）\n",
        "ts = corpus[1:]  # 入力語の次の語全部（1～1000）\n",
        "data_size = len(xs)  # 一見無駄な変数だが、corpusがcorpus_size分無い場合，data_sizeはcorpus_sizeと違う値になる（Pythonのリストのスライスは範囲外の値を指定してもエラーにならない）\n",
        "print(f'corpus size: {corpus_size}, vocabulary size: {vocab_size}')\n",
        "\n",
        "# 学習時に使用する変数\n",
        "max_iters = data_size // (batch_size * time_size)  # iterationごとに単語長time_sizeのバッチがbatch_size個処理される\n",
        "time_idx = 0\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "ppl_list = []  # perplexityを保存\n",
        "\n",
        "# モデルの生成\n",
        "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = SGD(lr)\n",
        "\n",
        "# ミニバッチを時間ずらして平行に作るためのoffsetsを計算\n",
        "jump = (corpus_size - 1) // batch_size  # バッチサイズで等分\n",
        "offsets = [i * jump for i in range(batch_size)]\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "  for iter in range(max_iters):\n",
        "    # ミニバッチの取得\n",
        "    batch_x = np.empty((batch_size, time_size), dtype='i')  # とりあえずbatch_size x time_sizeの整数値行列を初期化\n",
        "    batch_t = np.empty((batch_size, time_size), dtype='i')\n",
        "    for t in range(time_size):\n",
        "      for i, offset in enumerate(offsets):\n",
        "        batch_x[i, t] = xs[(offset + time_idx) % data_size]  # time_idxはこれまでのiteration回数*timesize+tの値\n",
        "        batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
        "      time_idx += 1\n",
        "    \n",
        "    # ミニバッチを順伝播\n",
        "    loss = model.forward(batch_x, batch_t)\n",
        "    # 逆伝播して重み更新\n",
        "    model.backward()\n",
        "    optimizer.update(model.params, model.grads)\n",
        "    total_loss += loss\n",
        "    loss_count += 1\n",
        "\n",
        "  # エポックごとにperplexity評価\n",
        "  ppl = np.exp(total_loss / loss_count)  # e^LのLの値にはこのエポックの損失値の平均を使う\n",
        "  print(f'| epoch {epoch+1} | perplexity {ppl:.2f}')\n",
        "  ppl_list.append(float(ppl))\n",
        "  total_loss, loss_count = 0, 0\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ptb.train.txt ... \n",
            "Done\n",
            "corpus size: 1000, vocabulary size: 418\n",
            "| epoch 1 | perplexity 370.01\n",
            "| epoch 2 | perplexity 256.26\n",
            "| epoch 3 | perplexity 225.31\n",
            "| epoch 4 | perplexity 216.99\n",
            "| epoch 5 | perplexity 206.85\n",
            "| epoch 6 | perplexity 202.89\n",
            "| epoch 7 | perplexity 198.61\n",
            "| epoch 8 | perplexity 196.60\n",
            "| epoch 9 | perplexity 192.39\n",
            "| epoch 10 | perplexity 193.08\n",
            "| epoch 11 | perplexity 188.46\n",
            "| epoch 12 | perplexity 191.67\n",
            "| epoch 13 | perplexity 190.04\n",
            "| epoch 14 | perplexity 190.16\n",
            "| epoch 15 | perplexity 188.40\n",
            "| epoch 16 | perplexity 185.42\n",
            "| epoch 17 | perplexity 182.37\n",
            "| epoch 18 | perplexity 179.93\n",
            "| epoch 19 | perplexity 180.90\n",
            "| epoch 20 | perplexity 183.02\n",
            "| epoch 21 | perplexity 179.35\n",
            "| epoch 22 | perplexity 176.31\n",
            "| epoch 23 | perplexity 174.15\n",
            "| epoch 24 | perplexity 174.25\n",
            "| epoch 25 | perplexity 175.61\n",
            "| epoch 26 | perplexity 171.65\n",
            "| epoch 27 | perplexity 170.86\n",
            "| epoch 28 | perplexity 168.73\n",
            "| epoch 29 | perplexity 164.28\n",
            "| epoch 30 | perplexity 158.02\n",
            "| epoch 31 | perplexity 161.00\n",
            "| epoch 32 | perplexity 154.19\n",
            "| epoch 33 | perplexity 152.90\n",
            "| epoch 34 | perplexity 148.43\n",
            "| epoch 35 | perplexity 150.35\n",
            "| epoch 36 | perplexity 140.65\n",
            "| epoch 37 | perplexity 136.84\n",
            "| epoch 38 | perplexity 134.20\n",
            "| epoch 39 | perplexity 128.48\n",
            "| epoch 40 | perplexity 124.93\n",
            "| epoch 41 | perplexity 122.92\n",
            "| epoch 42 | perplexity 121.56\n",
            "| epoch 43 | perplexity 112.00\n",
            "| epoch 44 | perplexity 111.60\n",
            "| epoch 45 | perplexity 105.26\n",
            "| epoch 46 | perplexity 104.93\n",
            "| epoch 47 | perplexity 99.53\n",
            "| epoch 48 | perplexity 92.47\n",
            "| epoch 49 | perplexity 89.15\n",
            "| epoch 50 | perplexity 87.23\n",
            "| epoch 51 | perplexity 82.64\n",
            "| epoch 52 | perplexity 78.84\n",
            "| epoch 53 | perplexity 74.61\n",
            "| epoch 54 | perplexity 71.31\n",
            "| epoch 55 | perplexity 68.71\n",
            "| epoch 56 | perplexity 64.51\n",
            "| epoch 57 | perplexity 60.39\n",
            "| epoch 58 | perplexity 56.97\n",
            "| epoch 59 | perplexity 53.96\n",
            "| epoch 60 | perplexity 50.78\n",
            "| epoch 61 | perplexity 50.20\n",
            "| epoch 62 | perplexity 46.49\n",
            "| epoch 63 | perplexity 42.91\n",
            "| epoch 64 | perplexity 40.85\n",
            "| epoch 65 | perplexity 39.28\n",
            "| epoch 66 | perplexity 37.42\n",
            "| epoch 67 | perplexity 35.79\n",
            "| epoch 68 | perplexity 32.55\n",
            "| epoch 69 | perplexity 30.86\n",
            "| epoch 70 | perplexity 29.84\n",
            "| epoch 71 | perplexity 28.60\n",
            "| epoch 72 | perplexity 26.49\n",
            "| epoch 73 | perplexity 24.92\n",
            "| epoch 74 | perplexity 23.23\n",
            "| epoch 75 | perplexity 22.66\n",
            "| epoch 76 | perplexity 21.10\n",
            "| epoch 77 | perplexity 19.86\n",
            "| epoch 78 | perplexity 18.76\n",
            "| epoch 79 | perplexity 17.26\n",
            "| epoch 80 | perplexity 16.21\n",
            "| epoch 81 | perplexity 16.07\n",
            "| epoch 82 | perplexity 15.81\n",
            "| epoch 83 | perplexity 14.11\n",
            "| epoch 84 | perplexity 14.11\n",
            "| epoch 85 | perplexity 12.91\n",
            "| epoch 86 | perplexity 12.58\n",
            "| epoch 87 | perplexity 11.42\n",
            "| epoch 88 | perplexity 10.54\n",
            "| epoch 89 | perplexity 10.32\n",
            "| epoch 90 | perplexity 9.55\n",
            "| epoch 91 | perplexity 9.37\n",
            "| epoch 92 | perplexity 9.20\n",
            "| epoch 93 | perplexity 8.71\n",
            "| epoch 94 | perplexity 8.04\n",
            "| epoch 95 | perplexity 7.71\n",
            "| epoch 96 | perplexity 7.12\n",
            "| epoch 97 | perplexity 6.66\n",
            "| epoch 98 | perplexity 6.40\n",
            "| epoch 99 | perplexity 6.17\n",
            "| epoch 100 | perplexity 5.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrf_tmAmSeoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}