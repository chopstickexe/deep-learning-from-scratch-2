{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch07-seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOLpog1C67XTp4Q8XIgCGqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chopstickexe/deep-learning-from-scratch-2/blob/master/ch07_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMKwPjfsSDLh",
        "colab_type": "text"
      },
      "source": [
        "# 下準備\n",
        "\n",
        "$$\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\mat}[1]{\\mathbf{#1}}\n",
        "$$\n",
        "\n",
        "## 数式の表記\n",
        "\n",
        "変数: 小文字イタリック $x$\n",
        "\n",
        "定数: 大文字イタリック $X$\n",
        "\n",
        "ベクトル: 小文字ローマン体太字 $\\vect{x}$\n",
        "\n",
        "行列: 大文字ローマン体太字 $\\mat{X}$\n",
        "\n",
        "## 公式実装のclone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpcQLFeCSHyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "9583beae-e8c8-4c20-dfe2-f48c3b4219be"
      },
      "source": [
        "!git clone --depth=1 https://github.com/oreilly-japan/deep-learning-from-scratch-2.git\n",
        "import sys \n",
        "sys.path.append('deep-learning-from-scratch-2')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 13), reused 14 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uL3zfaQAA7H",
        "colab_type": "text"
      },
      "source": [
        "# 7章 RNNによる文章生成\n",
        "\n",
        "6章で実装した言語モデルが出力する確率分布に従って単語をサンプリングし，文章を生成する．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7awCMjkQ1oo",
        "colab_type": "text"
      },
      "source": [
        "## 7.1 言語モデルを使った文章生成（`RnnlmGen`，`BetterRnnlmGen`クラス）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAJeMVW6Fiya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from common.functions import softmax\n",
        "from ch06.rnnlm import Rnnlm\n",
        "from ch06.better_rnnlm import BetterRnnlm\n",
        "\n",
        "class RnnlmGen(Rnnlm):\n",
        "  def generate(self, start_id, skip_ids=None, sample_size=100): \n",
        "    '''\n",
        "    start_id: 開始単語のID\n",
        "    skip_ids: <unk>など生成に使いたくない単語のリスト\n",
        "    sample_size=文長\n",
        "    '''\n",
        "    word_ids = [start_id]\n",
        "\n",
        "    x = start_id\n",
        "    while len(word_ids) < sample_size:\n",
        "      x = np.array(x).reshape(1, 1)\n",
        "      score = self.predict(x)  # Softmaxレイヤの直前の出力\n",
        "      p = softmax(score.flatten())  # flatten(): ndarrayを一次元の配列化する\n",
        "\n",
        "      sampled = np.random.choice(len(p), size=1, p=p)  # サンプリング．len(p)は生成されるサンプルの値域最大値，sizeは生成されるサンプル数，pはサンプルする際に使われる確率分布\n",
        "\n",
        "      if (skip_ids is None) or (sampled not in skip_ids):\n",
        "        x = sampled\n",
        "        word_ids.append(int(x))\n",
        "\n",
        "    return word_ids"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYlxeKMkzAkL",
        "colab_type": "text"
      },
      "source": [
        "PTBデータセットで学習した普通のLSTMモデルを利用して文章生成．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSGfdEN4Lzmg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "9c44abeb-420a-45ad-b107-5f3aeb6dcfb9"
      },
      "source": [
        "from dataset import ptb\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "model = RnnlmGen()\n",
        "model.load_params('deep-learning-from-scratch-2/ch06/Rnnlm.pkl')\n",
        "\n",
        "# start文字とskip文字の設定\n",
        "start_word = 'you'\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = ['N', '<unk>', '$']\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "\n",
        "# 文章生成\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "print(txt)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ptb.train.txt ... \n",
            "Done\n",
            "you lose a step we said it would submit to post new sources ad.\n",
            " for the wives process day under the slowdown champion beneath control plans to be eliminated to work standards said auditors stemming from epo but is rushing to edge four products.\n",
            " the scramble now noting providing warsaw 's share of fears more worse demand and tucson credits and any premiums engaged to solve clients of quality or shipment guest.\n",
            " but in recession the poll consortium is an outside consumer groups are reasonable or on the issue gasb to ensure why those developers remain fairly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AfIQxRf5_To",
        "colab_type": "text"
      },
      "source": [
        "LSTMレイヤを2層にして，各層にDropoutを入れ，さらにEmbeddingの転置行列（H x V, Hは入力単語を表現する隠れベクトルのサイズ，Vは元の語彙数）をAffine Layerの重みにそのまま使うモデル（`ch06/better_rnnlm.py`）に変更すると，生成される文章の質が上がる．\n",
        "（より言語のルールを守った文章になる）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7T-EwmDyz1G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7b93a658-d36a-4e1d-9dee-ac1a3098bb99"
      },
      "source": [
        "!curl -o deep-learning-from-scratch-2/ch06/BetterRnnlm.pkl https://www.oreilly.co.jp/pub/9784873118369/BetterRnnlm.pkl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 37.7M  100 37.7M    0     0  9222k      0  0:00:04  0:00:04 --:--:-- 9222k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VzlqYdky1Hf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "1ed2e93b-5c45-4e24-b839-69ea07e64699"
      },
      "source": [
        "from ch06.better_rnnlm import BetterRnnlm  # LSTMレイヤを2層利用して、各層でDropoutするモデル\n",
        "from ch07.rnnlm_gen import BetterRnnlmGen  # RnnlmGenの継承クラスをRnnlmからBetterRnnlmに変更しただけのクラス\n",
        "\n",
        "model = BetterRnnlmGen()\n",
        "model.load_params('deep-learning-from-scratch-2/ch06/BetterRnnlm.pkl')\n",
        "\n",
        "# start文字とskip文字の設定\n",
        "start_word = 'you'\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = ['N', '<unk>', '$']\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "\n",
        "# 文章生成\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "print(txt)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you are a last-minute drain to the court.\n",
            " but the brief for extra systems and the national institutes of health after a national institutes of health prohibit a sometimes strong role in the centerpiece of congress and its account for data council in school.\n",
            " new spy rooms.\n",
            " lawyers for pregnant women the network salesmen have illegally for private scientific suggest that he would do a good job in the k mart fla. ad agency.\n",
            " the dispute between westinghouse and mr. smith said the price of in big personnel are more limited by both consumers and magazines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8q46xdOuI25",
        "colab_type": "text"
      },
      "source": [
        "## 7.2 seq2seq（足し算問題）\n",
        "\n",
        "足し算seq2seqを実装するためのデータセットを確認．\n",
        "Decoder用の開始文字として`_`を入れている． "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SctJk0B9ue9L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "3bab89a1-d11e-4c82-b26f-ffb1c07ae64d"
      },
      "source": [
        "from dataset import sequence\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "  sequence.load_data('addition.txt', seed=1984)\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "print(x_train.shape, t_train.shape)\n",
        "print(x_test.shape, t_test.shape)\n",
        "\n",
        "print(x_train[0])\n",
        "print(t_train[0])\n",
        "\n",
        "print(''.join([id_to_char[c] for c in x_train[0]]))\n",
        "print(''.join([id_to_char[c] for c in t_train[0]]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 7) (45000, 5)\n",
            "(5000, 7) (5000, 5)\n",
            "[ 3  0  2  0  0 11  5]\n",
            "[ 6  0 11  7  5]\n",
            "71+118 \n",
            "_189 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpuQTRMPw3sG",
        "colab_type": "text"
      },
      "source": [
        "## 7.3 seq2seqの実装\n",
        "\n",
        "まずEncoderクラスを実装する．各時刻（各入力）はEmbeddingとLSTMの二層にする．\n",
        "\n",
        "Encoderクラスの出力として欲しいのは一番最後の時刻が出す隠れベクトルだけなので（ここ重要．最終時刻のセルも要らない），他の時刻のLSTMの隠れベクトルやセルは特に使わない．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2fSrjVaxS10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder:\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    rn = np.random.randn\n",
        "\n",
        "    embed_W = (rn(V, D) / 100).astype('f')\n",
        "    lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "    lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "    lstm_b = np.zeros(4 * H).astype('f')\n",
        "\n",
        "    self.embed = TimeEmbedding(embed_W)\n",
        "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
        "    \n",
        "    self.params = self.embed.params + self.lstm.params\n",
        "    self.grads = self.embed.grads + self.lstm.grads\n",
        "    self.hs = None\n",
        "\n",
        "  def forward(self, xs):\n",
        "    xs = self.embed.forward(xs)\n",
        "    hs = self.lstm.forward(xs)  # セルはそもそもforward()の戻り値として返って来ない仕様になっている\n",
        "    self.hs = hs\n",
        "    return hs[:, -1, :]  # 最終時刻のhのみ返す\n",
        "\n",
        "  def backward(self, dh):\n",
        "    dhs = np.zeros_like(self.hs)\n",
        "    dhs[:, -1, :] = dh  # 最終時刻のhの勾配をセットする\n",
        "\n",
        "    dout = self.lstm.backward(dhs)\n",
        "    dout = self.embed.backward(dout)\n",
        "    return dout"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3JDJOFL1xxR",
        "colab_type": "text"
      },
      "source": [
        "次にDecoderクラスを実装する．Encoderクラスから最終時刻の隠れベクトルを受け取り，RNNで文章を生成するが，今回は足し算の答えを出したいので，次の時刻の文字はSoftmaxに基づくサンプリングで決めるのではなく，一番高い確率値のものを決定的に出す．\n",
        "\n",
        "そのため，シンプルなRNNとして実装したEmbedding, LSTM, Affine, SoftmaxWithLossのうち，最後のSoftmaxWithLossを省略して，AffineまでをDecoderとする．（SoftmaxWithLoss部分は後で実装するSeq2seqクラスで扱う．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIZzmtf07uD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}