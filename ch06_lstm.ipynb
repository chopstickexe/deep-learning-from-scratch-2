{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch06-lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPwUPddnIzahu6uJNaQHZNh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chopstickexe/deep-learning-from-scratch-2/blob/master/ch06_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMKwPjfsSDLh",
        "colab_type": "text"
      },
      "source": [
        "# 下準備\n",
        "\n",
        "$$\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\mat}[1]{\\mathbf{#1}}\n",
        "$$\n",
        "\n",
        "## 数式の表記\n",
        "\n",
        "変数: 小文字イタリック $x$\n",
        "\n",
        "定数: 大文字イタリック $X$\n",
        "\n",
        "ベクトル: 小文字ローマン体太字 $\\vect{x}$\n",
        "\n",
        "行列: 大文字ローマン体太字 $\\mat{X}$\n",
        "\n",
        "## 公式実装のclone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpcQLFeCSHyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a0db874a-9310-4eb7-a8bb-6255a7e73ef8"
      },
      "source": [
        "!git clone --depth=1 https://github.com/oreilly-japan/deep-learning-from-scratch-2.git\n",
        "import sys \n",
        "sys.path.append('deep-learning-from-scratch-2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 13), reused 14 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es5ANDl6G6ry",
        "colab_type": "text"
      },
      "source": [
        "# 6章 ゲート付きRNN\n",
        "\n",
        "## 6.1 RNNの問題点（勾配消失・勾配爆発）\n",
        "\n",
        "5章のシンプルなRNNは時系列が長くなると，逆伝播の際にtanhの微分値\n",
        "\n",
        "$$ \n",
        "\\frac{\\delta \\mathrm{tanh}(\\vect{x})}{\\delta \\vect{x}} = 1 - \\mathrm{tanh}(\\vect{x})^2, (-1 \\leq \\mathrm{tanh}(\\vect{x}) \\leq 1)\n",
        "$$\n",
        "\n",
        "をtごとに損失値の勾配にかけ算することで勾配消失につながりやすくなる．\n",
        "\n",
        "また，同じ重み $\\mat{W_h}$ を損失値の勾配に何度もかけ算することで，勾配爆発につながりやすくなる．\n",
        "\n",
        "この勾配消失と勾配爆発の二つの問題のうち，勾配爆発は**勾配クリッピング**という比較的簡単な手法で抑えることができる．\n",
        "\n",
        "勾配クリッピングは以下のように計算する．あるイテレーションの結果，ニューラルネットワークのすべてのパラメータの勾配（いままでの実装で言うところの`self.grads`）を$\\hat{\\vect{g}}$としたとき，そのL2ノルム$||\\hat{\\vect{g}}||$があらかじめ定められた閾値$\\mathit{threshold}$を超えていたら，\n",
        "\n",
        "$$\n",
        "\\hat{\\vect{g}} = \\frac{\\mathit{threshold}}{||\\hat{\\vect{g}}||} \\hat{\\vect{g}}\n",
        "$$\n",
        "\n",
        "でパラメータの勾配を修正する．（勾配は必ず1倍以下の値になる）\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9zI0LrTtxN",
        "colab_type": "text"
      },
      "source": [
        "## 6.2 勾配消失とLSTM\n",
        "\n",
        "シンプルなRNNのもう一つの問題である勾配消失については以下のように解決する（これがいわゆるLSTM）．\n",
        "\n",
        "まず，$\\vect{h_{t-1}} \\mat{W_h} + \\vect{x_t} \\mat{W_x} + \\vect{b}$を必ずtanhに放り込むのではなく，別途直接次の時刻$\\mathrm{t+1}$に渡すルートを作る．（勾配消失の原因になるtanhをスキップ）\n",
        "このルートに相当する変数を**セル**（または記憶セル）と呼び，$\\vect{c_t}$で表現する．\n",
        "\n",
        "そして，以下の3種類のゲートを持たせる．\n",
        "\n",
        "- Outputゲート（$\\vect{o}$）: 出力する隠れベクトル$\\vect{h_t}$にどのくらい$\\tanh(\\vect{c_t})$を反映させるか決める\n",
        "- Forgetゲート（$\\vect{f}$）: $\\vect{c_{t-1}}$ をどのくらい $\\vect{c_t}$ に反映させるか決める\n",
        "- Inputゲート（$\\vect{i}$）: 従来のシンプルなRNNの出力$\\vect{g} = \\tanh(\\vect{h_{t-1}} \\mat{W_h} + \\vect{x_t} \\mat{W_x} + \\vect{b})$をどのくらい $\\vect{c_t}$ に反映させるか決める．\n",
        "\n",
        "これらのゲートの出力は0～1の係数になる．その値は，シンプルなRNNの出力と同様に，$\\vect{h_{t-1}}$と$\\vect{x_t}$から求める．具体的には以下のように，sigmoid関数（$\\sigma$）を用いて求める．（$\\otimes$はアダマール積を表し，ベクトルを要素ごとに積算する）\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\vect{f} &= \\sigma(\\vect{h_{t-1}} \\mat{W_h^{(\\vect{f})}} + \\vect{x_t} \\mat{W_x^{(\\vect{f})}} + \\vect{b}) \\\\\n",
        "\\vect{g} &= \\tanh(\\vect{h_{t-1}} \\mat{W_h^{(\\vect{g})}} + \\vect{x_t} \\mat{W_x^{(\\vect{g})}} + \\vect{b}) \\\\\n",
        "\\vect{i} &= \\sigma(\\vect{h_{t-1}} \\mat{W_h^{(\\vect{i})}} + \\vect{x_t} \\mat{W_x^{(\\vect{i})}} + \\vect{b}) \\\\\n",
        "\\vect{o} &= \\sigma(\\vect{h_{t-1}} \\mat{W_h^{(\\vect{o})}} + \\vect{x_t} \\mat{W_x^{(\\vect{o})}} + \\vect{b}) \\\\ \\\\\n",
        "\\vect{c_t} &= \\vect{f} \\otimes \\vect{c_{t-1}} + \\vect{i} \\otimes \\vect{g} \\\\\n",
        "\\vect{h_t} &= \\vect{o} \\otimes \\tanh(\\vect{c_t})  \n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIET5IV5xmMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}